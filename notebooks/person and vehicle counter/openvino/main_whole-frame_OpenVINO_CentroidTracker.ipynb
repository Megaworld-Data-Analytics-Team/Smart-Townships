{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68f9094",
   "metadata": {},
   "source": [
    "## Person and Vehicle Counter Using OpenCV and Pre-trained Intel Model by OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2ca2c",
   "metadata": {},
   "source": [
    "### NOTE: Work in Progress (currently includes Object Detection and Tracking)\n",
    "- Model reference: https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/person-vehicle-bike-detection-crossroad-0078"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdcdf76",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95fd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "root_path = \"c:/Users/Lenard/Person and Vehicle Counter/\" #set to directory of Person and Vehicle Counter repository\n",
    "sys.path += [root_path]\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "#for object tracking\n",
    "import dlib\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from pyimagesearch.trackableobject import TrackableObject\n",
    "\n",
    "import imutils\n",
    "from imutils.video import VideoStream, FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6ce9a",
   "metadata": {},
   "source": [
    "### Load the Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d1b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1024, Width: 1024\n",
      "data detection_out\n"
     ]
    }
   ],
   "source": [
    "THRESH = 0.8\n",
    "\n",
    "model_path = \"c:/Users/Lenard/intel/person-vehicle-bike-detection-crossroad-0078/FP16/person-vehicle-bike-detection-crossroad-0078.xml\"\n",
    "\n",
    "#initialize inference engine\n",
    "ie = Core()\n",
    "#read the network and corresponding weights from file\n",
    "detector = ie.read_model(model=model_path)\n",
    "\n",
    "#compile the model for the CPU (you may also use GPU, MYRIAD, etc.)\n",
    "#or let the engine choose the available device (AUTO)\n",
    "compiled_model = ie.compile_model(model=detector, device_name=\"GPU\")\n",
    "\n",
    "#get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "#get input size\n",
    "height, width = list(input_layer.shape)[2:]\n",
    "print(\"Height: {}, Width: {}\".format(height,width))\n",
    "print(input_layer.any_name, output_layer.any_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36010dce",
   "metadata": {},
   "source": [
    "### Load Centroid Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f521810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the centroid tracker, then initialize a list to store\n",
    "# every dlib correlation trackers, followed by a dictionary to\n",
    "# map every uniqued object ID to a TrackableObject\n",
    "#ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
    "ct = CentroidTracker()\n",
    "trackers = []\n",
    "trackableObjects = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66c461",
   "metadata": {},
   "source": [
    "### Processing Results\n",
    "- List available classes and create corresponding colors. \n",
    "- In post-processing, boxes are transformed via normalization. \n",
    "- NMS will also be used to avoid overlapping detections and those that do not meet the threshold.\n",
    "- Finally, boxes and labels will be drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b574ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes of person-vehicle-bike-detection-crossroad-0078\n",
    "classes = [\"person\", \"vehicle\", \"bike\"]\n",
    "\n",
    "#map corresponding colors to classes\n",
    "colors = cv2.applyColorMap(\n",
    "    src = np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap = cv2.COLORMAP_RAINBOW,\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7dd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(frame, results, thresh=0.6):\n",
    "    # size of the original frame\n",
    "    h, w = frame.shape[:2]\n",
    "    results = results.squeeze()\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    new_rects = []\n",
    "    for idx, label, score, xmin, ymin, xmax, ymax in results:\n",
    "        # create a box using pixel coordinates\n",
    "        boxes.append(\n",
    "            tuple(map(int, (xmin*w, ymin*h, (xmax-xmin)*w, (ymax-ymin)*h)))\n",
    "        )\n",
    "        labels.append(int(label))\n",
    "        scores.append(float(score))\n",
    "        ids.append(int(idx))\n",
    "        \n",
    "    # apply NMS (Non-Maximum Suppression) to eliminate overlapping entities\n",
    "    # this algorithm returns indices of objects to keep\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes, scores=scores, score_threshold=thresh, nms_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    #if there are no boxes\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "        \n",
    "    #filter detected objects\n",
    "    return [(labels[i], scores[i], boxes[i]) for i in indices.flatten()]\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    for label, score, box in boxes:\n",
    "        label = label-1\n",
    "        color = tuple(map(int, colors[label]))\n",
    "        #draw box\n",
    "        x2 = box[0] + box[2]\n",
    "        y2 = box[1] + box[3]\n",
    "        cv2.rectangle(frame, box[:2], (x2,y2), color=color, thickness=3)\n",
    "        \n",
    "        #draw label\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"{classes[label]} {score:.2f}\",\n",
    "            (box[0]+10, box[1]+30),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            frame.shape[1]/1000,\n",
    "            color,\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7458ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    vid = cv2.VideoCapture(source)\n",
    "\n",
    "    #give camera time to warm up\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    processing_times = collections.deque()\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    #begin video capturing\n",
    "    while (vid.isOpened()):\n",
    "        #capture frame\n",
    "        ret, frame = vid.read()\n",
    "        \n",
    "        if frame is None:\n",
    "            print('Completed!')\n",
    "            break\n",
    "        \n",
    "        #if frame is larger than full HD, reduce size to improve performance\n",
    "        scale = 1280 / max(frame.shape)\n",
    "        if scale < 1:\n",
    "            frame = cv2.resize(\n",
    "                src=frame,\n",
    "                dsize=None,\n",
    "                fx=scale,\n",
    "                fy=scale,\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # resize image to fit neural network input\n",
    "        input_frame = cv2.resize(\n",
    "            frame, dsize=(width,height), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        frame_rgb = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "        #create batch of images\n",
    "        input_frame = np.expand_dims(input_frame.transpose(2,0,1), axis=0)\n",
    "        \n",
    "        ## Initialize status\n",
    "        statyus = \"waiting\"\n",
    "        rects = []\n",
    "        \n",
    "        #perform detection on every 5 frames\n",
    "        if frame_count % 5 == 0:\n",
    "            # set status to detection\n",
    "            status = \"detecting\"\n",
    "            trackers = []\n",
    "            \n",
    "            #measure processing time\n",
    "            t1 = time.time()\n",
    "            #get detections\n",
    "            results = compiled_model([input_frame])[output_layer]\n",
    "            t2 = time.time()\n",
    "            \n",
    "            # size of the original frame\n",
    "            h, w = frame.shape[:2]\n",
    "            results_squeezed = results.squeeze()\n",
    "    \n",
    "            for idx, label, score, xmin, ymin, xmax, ymax in results_squeezed:\n",
    "                \n",
    "                if score > THRESH:\n",
    "                    #get (x,y) coordinates of bounding box\n",
    "                    (startX, startY, endX, endY) = tuple(map(int, (xmin*w, ymin*h, xmax*w, ymax*h)))\n",
    "                    \n",
    "                    #start dlib correlation tracker\n",
    "                    tracker = dlib.correlation_tracker()\n",
    "                    rect = dlib.rectangle(startX, startY, endX, endY)\n",
    "                    tracker.start_track(frame, rect)\n",
    "                    \n",
    "                    #add the tracker to tracker list\n",
    "                    trackers.append(tracker)\n",
    "        else:\n",
    "            for tracker in trackers:\n",
    "                #set status to tracking\n",
    "                status = \"tracking\"\n",
    "                \n",
    "                #update tracker\n",
    "                tracker.update(frame)\n",
    "                pos = tracker.get_position()\n",
    "                \n",
    "                #Unpack the position object\n",
    "                startX = int(pos.left())\n",
    "                startY = int(pos.top())\n",
    "                endX = int(pos.right())\n",
    "                endY = int(pos.bottom())\n",
    "                \n",
    "                #append to rects list\n",
    "                rects.append((startX, startY, endX, endY))\n",
    "        \n",
    "        # get poses from network results\n",
    "        boxes = process_results(frame=frame, results=results, thresh=THRESH)\n",
    "        # draw bboxes\n",
    "        frame = draw_boxes(frame=frame, boxes=boxes)\n",
    "        \n",
    "        #use centroid tracker to associate old object centroids with new ones\n",
    "        objects = ct.update(rects)\n",
    "        \n",
    "        #loop over tracked objects\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            #check for an existing object ID for a trackable object\n",
    "            to = trackableObjects.get(objectID, None)\n",
    "            \n",
    "            #if there is no existing trackable object, create one\n",
    "            if to is None:\n",
    "                to = TrackableObject(objectID, centroid)\n",
    "                \n",
    "            else:\n",
    "                to.centroids.append(centroid)\n",
    "                \n",
    "            #store trackable object in dictionary\n",
    "            trackableObjects[objectID] = to\n",
    "            \n",
    "            #display text and circle relating to the tracked object\n",
    "            cv2.putText(\n",
    "                frame, \"ID {}\".format(objectID),\n",
    "                (centroid[0]-10, centroid[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n",
    "            )\n",
    "            cv2.circle(frame, (centroid[0],centroid[1]), 4, (0,255,0), -1)\n",
    "        \n",
    "        processing_times.append(t1 - t2)\n",
    "        #use processing times from last 200 frames\n",
    "        if len(processing_times) > 200:\n",
    "            processing_times.popleft()\n",
    "            \n",
    "        _, f_width = frame.shape[:2]\n",
    "        # get mean processing time [ms]\n",
    "        processing_time = np.mean(processing_times) * 1000\n",
    "        #get fps\n",
    "        fps = 1000 / processing_time\n",
    "        #display fps\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Inference Time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "            (20, 40),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            f_width / 1000,\n",
    "            (0,0,255),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        #display video stream\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press Q to quit\n",
    "            break\n",
    "\n",
    "    #clear stream capture\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2d4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = root_path + 'data/video/test.mp4'\n",
    "detect(source=video_path, flip=True, use_popup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00d495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
