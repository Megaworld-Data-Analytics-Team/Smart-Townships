{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2a10b1",
   "metadata": {},
   "source": [
    "## Improved Application for Demographics and Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f264852",
   "metadata": {},
   "source": [
    "### NOTE: Don't Forget to Activate OpenVINO Runtime\n",
    "1. Open Command Line\n",
    "2. Execute openvino_env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4eb6b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2b98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import imutils\n",
    "from operator import itemgetter\n",
    "\n",
    "root_path = \"c:/Users/Lenard/Person and Vehicle Counter/\" #set to directory of Person and Vehicle Counter repository\n",
    "sys.path += [root_path]\n",
    "\n",
    "import cv2\n",
    "import dnn\n",
    "import dlib\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from pyimagesearch.trackableobject import TrackableObject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fadde",
   "metadata": {},
   "source": [
    "### Define Video Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab97d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_W = 640 # 1280  # 640  # 256  # 320  # 480\n",
    "RES_H = 480 # 960   # 480  # 192  # 240  # 360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda09cb",
   "metadata": {},
   "source": [
    "### Get Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2ce92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 0.8 #minimum confidence score\n",
    "\n",
    "detector = cv2.FaceDetectorYN.create(\n",
    "        \"face_detection_yunet_2022mar.onnx\",\n",
    "        \"\",\n",
    "        (RES_W, RES_H),\n",
    "        THRESH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ced1c9",
   "metadata": {},
   "source": [
    "### Get Classification Models (from Open Model Zoo/Intel Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf86d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Labels\n",
    "emotion_labels = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "gender_labels = [\"female\", \"male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0daae350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "#Age and Gender Model\n",
    "#NOTE: change to path of saved model\n",
    "age_gender_model = ie.read_model(model=\"c:/Users/Lenard/intel/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml\")\n",
    "compiled_age_gender_model = ie.compile_model(model=age_gender_model, device_name=\"GPU\")\n",
    "gender_output_layer = compiled_age_gender_model.outputs[0]\n",
    "age_output_layer = compiled_age_gender_model.outputs[1]\n",
    "age_gender_input_layer = next(iter(compiled_age_gender_model.inputs))\n",
    "\n",
    "#Emotion Model\n",
    "#NOTE: change to path of saved model\n",
    "emotion_model = ie.read_model(model=\"c:/Users/Lenard/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml\")\n",
    "compiled_emotion_model = ie.compile_model(model=emotion_model, device_name=\"GPU\") \n",
    "emotion_output_layer = next(iter(compiled_emotion_model.outputs))\n",
    "emotion_input_layer = next(iter(compiled_emotion_model.inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a189875",
   "metadata": {},
   "source": [
    "### Get Centroid Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c751068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
    "trackers = []\n",
    "trackableObjects={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0479a",
   "metadata": {},
   "source": [
    "### Track Using Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbacda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "#give camera time to warm up\n",
    "time.sleep(0.1)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "trackers = []\n",
    "track_ids = []\n",
    "\n",
    "#begin video capturing\n",
    "while (vid.isOpened()):\n",
    "    #capture frame\n",
    "    ret, frame = vid.read()\n",
    "    \n",
    "    emotion_color = (0, 0, 0)\n",
    "    gender_color = (0, 0, 0)\n",
    "    \n",
    "    if ret == True:\n",
    "    \n",
    "        #resize frame\n",
    "        frame = imutils.resize(frame, width=RES_W, height=RES_H)\n",
    "        image = frame\n",
    "\n",
    "        # Loop over frames from the video stream\n",
    "        # Obtain the raw numpy array representing the image, then initialize the timestamp\n",
    "        # and the occupied/unoccupied text\n",
    "        image_copy = np.copy(image)\n",
    "        \n",
    "        # start time for FPS throughput estimator\n",
    "        t1 = time.time()\n",
    "        \n",
    "        #set status to waiting\n",
    "        status = 'waiting'\n",
    "        rects = []\n",
    "        \n",
    "        \n",
    "        #On first frame\n",
    "        if frame_count % 10 == 0:\n",
    "            # set status and initialize our new set of object trackers\n",
    "            status = 'detecting'\n",
    "            trackers = []\n",
    "            \n",
    "            #detect\n",
    "            detector.setInputSize((RES_W, RES_H))\n",
    "            bboxes = detector.detect(image_copy)\n",
    "            \n",
    "            # Loop through list and overlay green bboxes\n",
    "            for idx, bbox in enumerate(bboxes[1]):\n",
    "                #Convert coordinates from float to int\n",
    "                coords = bbox[:-1].astype(np.int32)\n",
    "                track_ids.append(idx)\n",
    "                #display bbox\n",
    "                cv2.rectangle(image_copy, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0,255,0), 3)\n",
    "                (startX, startY, endX, endY) = (coords[0], coords[1], coords[0]+coords[2], coords[1]+coords[3])\n",
    "                \n",
    "                # Initialize the tracker on the first frame\n",
    "                # Create the correlation tracker (needs initialization priort to usage)\n",
    "                tracker = dlib.correlation_tracker()\n",
    "                \n",
    "                # Begin a track on face detected on first frame\n",
    "                rect = dlib.rectangle(coords[0], coords[1], coords[0]+coords[2], coords[1]+coords[3])\n",
    "                tracker.start_track(image_copy, rect)\n",
    "                \n",
    "                #Add the tracker to list of trackers for utilization in skipping frames\n",
    "                trackers.append(tracker)\n",
    "        \n",
    "        #On subsequent frames\n",
    "        else:\n",
    "            #Allow tracking from the previous frame (tracks all detected faces)\n",
    "            for track_id, tracker in zip(track_ids, trackers):\n",
    "                tracker.update(image_copy)\n",
    "                pos = tracker.get_position()\n",
    "                \n",
    "                #Unpack the position object\n",
    "                startX = int(pos.left())\n",
    "                startY = int(pos.top())\n",
    "                endX = int(pos.right())\n",
    "                endY = int(pos.bottom())\n",
    "                \n",
    "                #append to rects list\n",
    "                rects.append((startX, startY, endX, endY))\n",
    "                \n",
    "                #Get ROI for classification\n",
    "                roi_color = image_copy[startY:(startY+endY), startX:(startX+endX)]\n",
    "                roi_emotion = cv2.resize(roi_color, (64,64), interpolation=cv2.INTER_AREA)\n",
    "                roi_age_gender = cv2.resize(roi_color, (62,62), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                input_emotion = np.expand_dims(roi_emotion.transpose(2,0,1), axis=0) #expand dimensions for proper prediction (N,C,H,W)\n",
    "                \n",
    "                input_age_gender = np.expand_dims(roi_age_gender.transpose(2,0,1), axis=0) #expand dimensions for proper prediction (N,C,H,W)\n",
    "\n",
    "                \n",
    "                #Age Prediction\n",
    "                age_predict = compiled_age_gender_model(inputs=[input_age_gender])[age_output_layer]\n",
    "                age_label = int(age_predict*100)\n",
    "                cv2.putText(image_copy, \"Age: \" + str(age_label), (startX, startY-70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                \n",
    "                \n",
    "                #Gender Prediction\n",
    "                gender_predict = compiled_age_gender_model(inputs=[input_age_gender])[gender_output_layer]\n",
    "                gender_index = np.argmax(gender_predict)\n",
    "                gender_label = gender_labels[gender_index]\n",
    "                if gender_label == \"male\":\n",
    "                    gender_color = (255, 0, 0)\n",
    "                elif gender_label == \"female\":\n",
    "                    gender_color = (0, 0, 255)\n",
    "                cv2.putText(image_copy, \"Sex: \" + str(gender_label), (startX, startY-40), cv2.FONT_HERSHEY_SIMPLEX, 1, gender_color, 2)\n",
    "                \n",
    "                \n",
    "                #Emotion Prediction\n",
    "                emotion_predict = compiled_emotion_model(inputs=[input_emotion])[emotion_output_layer]\n",
    "                emotion_index = np.argmax(emotion_predict)\n",
    "                emotion_label = emotion_labels[emotion_index]\n",
    "                \n",
    "                if emotion_label == 'happy' or emotion_label == 'surprise':\n",
    "                    emotion_color = (0, 255, 0)\n",
    "                elif emotion_label == 'neutral':\n",
    "                    emotion_color = (255, 0, 0)\n",
    "                else:\n",
    "                    emotion_color = (0, 0, 255)\n",
    "                cv2.putText(image_copy, emotion_label, (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1, emotion_color, 2)\n",
    "                \n",
    "                \n",
    "                #Draw the bounding box for the tracker\n",
    "                cv2.putText(image_copy, \"Person - \" + str(track_id), (startX, startY-100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "                cv2.rectangle(image_copy, (startX, startY), (endX, endY), (0,255,0), 3)\n",
    "        \n",
    "        #use centroid tracker\n",
    "        objects = ct.update(rects)\n",
    "        \n",
    "        #loop over tracked objects\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            #check for an existing object ID for a trackable object\n",
    "            to = trackableObjects.get(objectID, None)\n",
    "            \n",
    "            #if there is no existing trackable object, create one\n",
    "            if to is None:\n",
    "                to = TrackableObject(objectID, centroid)\n",
    "                \n",
    "            else:\n",
    "                to.centroids.append(centroid)\n",
    "                \n",
    "            #store trackable object in dictionary\n",
    "            trackableObjects[objectID] = to\n",
    "            \n",
    "            #display text and circle relating to the tracked object\n",
    "            cv2.putText(\n",
    "                image_copy, \"ID {}\".format(objectID),\n",
    "                (centroid[0]-10, centroid[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n",
    "            )\n",
    "            cv2.circle(image_copy, (centroid[0],centroid[1]), 4, (0,255,0), -1)\n",
    "            \n",
    "        \n",
    "        #Increment frame count\n",
    "        frame_count += 1\n",
    "        \n",
    "        #display video stream\n",
    "        cv2.imshow('Video', image_copy)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #press Q to quit\n",
    "            break\n",
    "\n",
    "#clear stream capture\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58540cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
