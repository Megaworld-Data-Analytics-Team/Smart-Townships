{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be8ed12",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import FPS\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import deque\n",
    "\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdea8d7",
   "metadata": {},
   "source": [
    "### Helper Functions for Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tracker(bbox, label, rgb, inputQueue, outputQueue):\n",
    "    # construct a dlib rectangle object from the bbox\n",
    "    # coordinates and then initiate the correlation tracker\n",
    "    tracker = dlib.correlation_tracker()\n",
    "    rect = dlib.rectangle(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    tracker.start_track()\n",
    "    \n",
    "    # loop indefinitely\n",
    "    while True:\n",
    "        # attempt to get the next frame from the input queue\n",
    "        rgb = inputQueue.get()\n",
    "        \n",
    "        # if there is an entry in the queue, process it\n",
    "        if rgb is not None:\n",
    "            # update the tracker and get the position of the tracked object\n",
    "            tracker.update(rgb)\n",
    "            pos = tracker.get_position()\n",
    "            \n",
    "            # unpack the position object\n",
    "            startX = int(pos.left())\n",
    "            startY = int(pos.top())\n",
    "            endX = int(pos.right())\n",
    "            endY = int(pos.bottom())\n",
    "            \n",
    "            # include the label & bbox coordinates to the output queue\n",
    "            outputQueue.put((label, (startX, startY, endX, endY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a148ace",
   "metadata": {},
   "source": [
    "### Load MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MobileNet models\n",
    "prototxt = './mobilenet_ssd/MobileNetSSD_deploy.prototxt'\n",
    "model = './mobilenet_ssd/MobileNetSSD_deploy.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of class labels that MobileNet SSD was trained in\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "allowed_classes = [\"bicycle\", \"bus\", \"car\",\"motorbike\", \"person\"]\n",
    "\n",
    "#map corresponding colors to classes\n",
    "colors = np.random.uniform(255, 0, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1833ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from disk\n",
    "print(\"[INFO] Loading MobileNet SSD model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bd6de",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence threshold\n",
    "CONF = 0.5\n",
    "\n",
    "# initialize the frame dimensions (will be set once the first frame is read)\n",
    "W, H = None, None\n",
    "\n",
    "# initialize our queues (input queue and output queue) for each object to be tracked\n",
    "inputQueues = []\n",
    "outputQueues = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b056b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers = []\n",
    "trackableObjects = {}\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28744c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [deque(maxlen=30) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for counting totals\n",
    "person_counter = []\n",
    "vehicle_counter = []\n",
    "\n",
    "#for counting in hours\n",
    "person_counter_hour = []\n",
    "vehicle_counter_hour = []\n",
    "\n",
    "#for storing the count of the previous hours\n",
    "prev_hours_person_count = 0\n",
    "prev_hours_vehicle_count = 0\n",
    "\n",
    "#dictionary for count data\n",
    "#define dictionary of data\n",
    "count_dict = {'Total Persons': [0], 'Total Vehicles': [0], 'Day': [0], 'Date': [0], 'Time': [0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6dc342",
   "metadata": {},
   "source": [
    "### Track Using Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectTracker = {}\n",
    "objectClass = {}\n",
    "currentObjectID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define video path\n",
    "video = './data/video/test.mp4'\n",
    "\n",
    "# write to output?\n",
    "output = False\n",
    "\n",
    "# Initialize the video stream and output video writer\n",
    "print(\"[INFO] Starting video stream...\")\n",
    "vid = cv2.VideoCapture(video)\n",
    "\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    " \n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "\n",
    "writer = None\n",
    "\n",
    "# start FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "frame_count = 0\n",
    "skip_frames = 10\n",
    "\n",
    "# loop over frames in video stream\n",
    "while (vid.isOpened()):\n",
    "    # get the next frame from the video\n",
    "    ret, frame = vid.read()\n",
    "    \n",
    "    # check if we have reached the end of the video\n",
    "    if frame is None:\n",
    "        print(\"Completed!\")\n",
    "        break\n",
    "    \n",
    "    new_frame_time = time.time()\n",
    "    \n",
    "    # Perform frame preprocesing\n",
    "    # resize the frame and convert from BGR to RGB\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "        \n",
    "    objectIDtoDelete = []\n",
    "    \n",
    "    for objectID in objectTracker.keys():\n",
    "        trackingQuality = objectTracker[objectID].update(frame)\n",
    "        if trackingQuality < 7:\n",
    "            objectIDtoDelete.append(objectID)\n",
    "    \n",
    "    for objectID in objectIDtoDelete:\n",
    "        print('Removing object ID ' + str(objectID) + ' from list of trackers.')\n",
    "        objectTracker.pop(objectID, None)\n",
    "    \n",
    "    # *include write to output video if necessary*\n",
    "    \n",
    "    # if our queue is empty then it's an indication to create our first object tracker\n",
    "    # check to see if we should run a more computationally expensive object detection method to aid our tracker\n",
    "    if frame_count % skip_frames == 0:\n",
    "        \n",
    "        # get frame dimensions and convert the frame to a blob\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.007843, (w,h), 127.5)\n",
    "        \n",
    "        # input the blob to the model and get the detections/predictions\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        # loop over the detections\n",
    "        for i in np.arange(0, detections.shape[2]):\n",
    "            # extract the confidence score associated with the prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            # only include detections that pass the CONF threshold\n",
    "            if confidence > CONF:\n",
    "                # extract the index of the class label from the detection\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "                label = CLASSES[idx]\n",
    "                \n",
    "                # if the class label is not a person, ignore it\n",
    "                if CLASSES[idx] not in allowed_classes:\n",
    "                    continue\n",
    "                \n",
    "                # compute the (x,y) coordinates of the bounding box for the object\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype('int')\n",
    "                \n",
    "                center = ((startX+endX)/2, (startY+endY)/2)\n",
    "        \n",
    "                matchObjectID = None\n",
    "        \n",
    "                for objectID in objectTracker.keys():\n",
    "                    trackedPosition = objectTracker[objectID].get_position()\n",
    "                    \n",
    "                    # unpack the position object\n",
    "                    t_startX = int(trackedPosition.left())\n",
    "                    t_startY = int(trackedPosition.top())\n",
    "                    t_endX = int(trackedPosition.right())\n",
    "                    t_endY = int(trackedPosition.bottom())\n",
    "                    \n",
    "                    t_center = ((t_startX+t_endX)/2, (t_startY+t_endY)/2)\n",
    "                    \n",
    "                    if ( (t_startX <= center[0] <= t_endX) and (t_startY <= center[1] <= t_endY) and (startX <= t_center[0] <= endX) and (startY <= t_center[1] <= endY) ):\n",
    "                        matchObjectID = objectID\n",
    "                \n",
    "                #create new object tracker if nonexistent for an object\n",
    "                if matchObjectID is None:\n",
    "                    print('Creating new tracker for ID ' + str(currentObjectID))\n",
    "                    \n",
    "                    # construct a dlib rectangle object from the\n",
    "                    # bounding box coordinates and start the correlation tracker\n",
    "                    tracker = dlib.correlation_tracker()\n",
    "                    rect = dlib.rectangle(startX, startY, endX, endY)\n",
    "                    tracker.start_track(frame_rgb, rect)\n",
    "                    \n",
    "                    objectTracker[currentObjectID] = tracker\n",
    "                    objectClass[currentObjectID] = label\n",
    "                    \n",
    "                    currentObjectID+=1 \n",
    "    \n",
    "    person_current_count = int(0) #detect current vehicle in specific zone\n",
    "    vehicle_current_count = int(0) #detect current vehicles in specific zone\n",
    "    \n",
    "    #display tracked objects     \n",
    "    for objectID in objectTracker.keys():\n",
    "        trackedPosition = objectTracker[objectID].get_position()\n",
    "        class_name = objectClass[objectID]\n",
    "\n",
    "        # unpack the position object\n",
    "        t_startX = int(trackedPosition.left())\n",
    "        t_startY = int(trackedPosition.top())\n",
    "        t_endX = int(trackedPosition.right())\n",
    "        t_endY = int(trackedPosition.bottom())\n",
    "        \n",
    "        #assign colors\n",
    "        color = colors[CLASSES.index(class_name)]\n",
    "        \n",
    "        cv2.putText(frame, \"ID {}\".format(objectID), (t_startX,t_startY-15), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 2)\n",
    "        cv2.putText(frame, class_name, (t_startX,t_endY+15), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), )\n",
    "        cv2.rectangle(frame, (t_startX,t_startY), (t_endX,t_endY), color, 2)\n",
    "        \n",
    "        center = (int((startX + endX)/2), int((startY + endY)/2)) #get center coordinates of bounding box\n",
    "        \n",
    "        #for counting\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if center[1] <= h and center[1] >= 0 and center[0] >= 0 and center[0] <= w:\n",
    "            if class_name == allowed_classes[-1]: #if detected class is a person\n",
    "                if int(objectID) not in person_counter:\n",
    "                    person_counter.append(int(objectID))\n",
    "                person_current_count += 1\n",
    "            elif class_name in allowed_classes[:-1]: #if detected class is a vehicle or bike\n",
    "                if int(objectID) not in vehicle_counter:\n",
    "                    vehicle_counter.append(int(objectID))\n",
    "                vehicle_current_count += 1\n",
    "    \n",
    "    #display persons count\n",
    "    person_total_count = len(set(person_counter))\n",
    "    person_total_count_hour = len(set(person_counter)) - prev_hours_person_count\n",
    "    cv2.putText(frame, \"Current Persons in Frame: \" + str(person_current_count), (20, 60),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Total Persons Detected This Hour: \" + str(person_total_count_hour), (20, 80),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Total Persons Detected: \" + str(person_total_count), (20, 100),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    #display vehicle count\n",
    "    vehicle_total_count = len(set(vehicle_counter))\n",
    "    vehicle_total_count_hour = len(set(vehicle_counter)) - prev_hours_vehicle_count\n",
    "    cv2.putText(frame, \"Current Vehicles in Frame: \" + str(vehicle_current_count), (20, 140),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Total Vehicles Detected This Hour: \" + str(vehicle_total_count_hour), (20, 160),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Total Vehicles Detected: \" + str(vehicle_total_count), (20, 180),cv2.FONT_HERSHEY_COMPLEX, W / 1500, (0,255,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #Save count every hour\n",
    "    if ( (time.localtime(time.time()).tm_min % 1 == 0) and (time.localtime(time.time()).tm_min == 0) and (time.localtime(time.time()).tm_sec == 0) ):\n",
    "        #append to dictionary\n",
    "        date_time_split = current_time.split()\n",
    "        count_dict['Total Persons'].append(person_total_count_hour)\n",
    "        count_dict['Total Vehicles'].append(vehicle_total_count_hour)\n",
    "        count_dict['Day'].append(date_time_split[0])\n",
    "        count_dict['Date'].append(date_time_split[2] + \" \" + date_time_split[1] + \" \" + date_time_split[4])\n",
    "        count_dict['Time'].append(date_time_split[3])\n",
    "\n",
    "        #form dataframe\n",
    "        df = pd.DataFrame(count_dict)\n",
    "\n",
    "        #save dataframe to CSV file\n",
    "        df.to_csv('Person and Vehicle Count (Per Hour).csv')\n",
    "        print('Saved Count Data for ' + current_time)\n",
    "\n",
    "        #reset total person and vehicle count\n",
    "        prev_hours_person_count = person_total_count\n",
    "        prev_hours_vehicle_count = vehicle_total_count\n",
    "        \n",
    "        time.sleep(5)\n",
    "    \n",
    "    #display fps\n",
    "    if new_frame_time - prev_frame_time != 0:\n",
    "        curr_fps = 1/(new_frame_time-prev_frame_time)\n",
    "        cv2.putText(frame, \"FPS: {:.2f}\".format(curr_fps), (20, 40),cv2.FONT_HERSHEY_COMPLEX, W / 1000, (0,0,255), 1, cv2.LINE_AA)\n",
    "    prev_frame_time = new_frame_time\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # press Q to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "    \n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] Approximate FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# cleanup video\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faa324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
