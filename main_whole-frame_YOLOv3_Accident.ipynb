{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad71f081",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb13b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For command line use of YOLOv3 (required)\n",
    "import collections\n",
    "from collections import deque\n",
    "from absl import flags\n",
    "import sys\n",
    "FLAGS = flags.FLAGS\n",
    "sys.argv = sys.argv[:1]\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "#for writing data to a csv file\n",
    "import pandas as pd\n",
    "\n",
    "import time #for calculating FPS\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2 #OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Use the GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#Check if GPU is being used\n",
    "print(\"GPU: \", tf.test.is_gpu_available())\n",
    "\n",
    "#under yolov3_tf2 folder\n",
    "from yolov3_tf2.models import YoloV3\n",
    "from yolov3_tf2.dataset import transform_images #for data augmentation\n",
    "from yolov3_tf2.utils import convert_boxes, load_darknet_weights, preprocess_image #converts bboxes to deepsort format\n",
    "\n",
    "#for object tracking\n",
    "from deep_sort import preprocessing #for max suppressions\n",
    "from deep_sort import nn_matching #for setting up the association metrics\n",
    "from deep_sort.detection import Detection #for object detection\n",
    "from deep_sort.tracker import Tracker #for object tracking information\n",
    "from tools import generate_detections as gdet #feature generation encoder\n",
    "\n",
    "# for detecting anomaly in acceleration\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pysad.models.integrations import ReferenceWindowModel\n",
    "from pysad.utils import ArrayStreamer\n",
    "from pysad.transform.postprocessing import RunningAveragePostprocessor\n",
    "from pysad.transform.preprocessing import InstanceUnitNormScaler\n",
    "from pysad.transform.probability_calibration import GaussianTailProbabilityCalibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad576a3",
   "metadata": {},
   "source": [
    "### Load YOLOv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define classes\n",
    "class_names = [c.strip() for c in open('./data/labels/coco.names').readlines()]\n",
    "\n",
    "#define allowed classes:\n",
    "allowed_classes = ['person', 'bicycle', 'car', 'motorbike', 'bus', 'truck']\n",
    "vehicle_classes = ['bicycle', 'car', 'motorbike', 'bus', 'truck']\n",
    "\n",
    "#define paths\n",
    "weightsyolov3 = './weights/yolov3.weights' # path to weights file\n",
    "weights= 'checkpoints/yolov3.tf' # path to checkpoints file\n",
    "size= 416             #resize images to\\\n",
    "checkpoints = 'checkpoints/yolov3.tf'\n",
    "num_classes = 80      # number of classes in the model\n",
    "\n",
    "#load model\n",
    "yolo = YoloV3(classes=num_classes)\n",
    "load_darknet_weights(yolo, weightsyolov3)\n",
    "yolo.save_weights(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0ae0d",
   "metadata": {},
   "source": [
    "### Load DeepSORT Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.4 #used to determine if objects between frames are the same\n",
    "nn_budget = None #used to form a gallery for storing of features\n",
    "nms_max_overlap = 0.5 #used to avoid too many detections on the same object\n",
    "\n",
    "model_filename = './model_data/mars-small128.pb' #pretrained CNN for pedestrian tracking\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=8) #feature generations\n",
    "\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget) #for measuring associations\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72467aa",
   "metadata": {},
   "source": [
    "### Load Models for Vehicle Acceleration Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = ArrayStreamer(shuffle=False) # Streamer to simulate streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = InstanceUnitNormScaler() # Normalizing\n",
    "postprocessor = RunningAveragePostprocessor(window_size=5) # Running average postprocessor\n",
    "calibrator = GaussianTailProbabilityCalibrator(running_statistics=True, window_size=6) # Probability calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b040c7",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762abb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get intersection of two lines\n",
    "def line_intersection(line1, line2):\n",
    "    # Line 1 represented as a1x + b1y = c1\n",
    "    a1 = line1[1][1] - line1[0][1]\n",
    "    b1 = line1[0][0] - line1[1][0]\n",
    "    c1 = a1*(line1[0][0]) + b1*(line1[0][1])\n",
    " \n",
    "    # Line 2 represented as a2x + b2y = c2\n",
    "    a2 = line2[1][1] - line2[0][1]\n",
    "    b2 = line2[0][0] - line2[1][0]\n",
    "    c2 = a2*(line2[0][0]) + b2*(line2[0][1])\n",
    " \n",
    "    determinant = a1*b2 - a2*b1\n",
    " \n",
    "    if (determinant == 0):\n",
    "        x = 0\n",
    "        y = 0\n",
    "    else:\n",
    "        x = (b2*c1 - b1*c2)/determinant\n",
    "        y = (a1*c2 - a2*c1)/determinant\n",
    "    \n",
    "    return int(x), int(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83fb7f",
   "metadata": {},
   "source": [
    "### Track Using Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48562d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to select video\n",
    "vid = cv2.VideoCapture('./data/video/Megaworld CCTV/megaworld_accident_1.MP4') #vehicles parallel to one another\n",
    "# vid = cv2.VideoCapture('./data/video/Megaworld CCTV/megaworld_accident_2.MP4') #camera is too far\n",
    "# vid = cv2.VideoCapture('./data/video/YouTube Accidents/youtube_accident_1.mp4') #india accident video\n",
    "# vid = cv2.VideoCapture('./data/video/YouTube Accidents/youtube_accident_2.mp4') #video with the best angle\n",
    "\n",
    "#give video time to warm up\n",
    "time.sleep(0.1)\n",
    "\n",
    "processing_times = deque()\n",
    "\n",
    "#list for historical trajectory\n",
    "track_bboxes = [deque(maxlen=30) for _ in range(1000)]\n",
    "points = [deque(maxlen=30) for _ in range(1000)]\n",
    "speeds = [deque(maxlen=30) for _ in range(1000)]\n",
    "accelerations = [deque(maxlen=30) for _ in range(1000)]\n",
    "acc_anomaly_list = [deque(maxlen=30) for _ in range(1000)]\n",
    "\n",
    "track_ids = []\n",
    "overlap_list = []\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "overlap_anomaly = False\n",
    "acc_anomaly = False\n",
    "angle_anomaly = True\n",
    "collision = False\n",
    "\n",
    "#begin video capturing\n",
    "while (vid.isOpened()):\n",
    "    #capture frame\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if frame is None:\n",
    "        print('Completed!')\n",
    "        break\n",
    "\n",
    "    #if frame is larger than full HD, reduce size to improve performance\n",
    "    scale = 1280 / max(frame.shape)\n",
    "    if scale < 1:\n",
    "        frame = cv2.resize(\n",
    "            src=frame,\n",
    "            dsize=None,\n",
    "            fx=scale,\n",
    "            fy=scale,\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        \n",
    "    # frame parameters\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    \n",
    "    #preprocessing for YOLOv3 Input\n",
    "    frame_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #video captured by OpenCV is in BGR format; tensorflow is RGB\n",
    "    frame_input = tf.expand_dims(frame_input, 0) #expands dims from C,H,W to N,C,H,W\n",
    "    frame_input = preprocess_image(frame_input, 416) #tensorflow shape is 416\n",
    "    \n",
    "    #start the timer\n",
    "    t1 = time.time() \n",
    "    \n",
    "    #Get detection\n",
    "    bboxes, scores, classes, nums = yolo(frame_input)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    #maximum of 100 bboxes per image\n",
    "    #boxes: 3D shape (1, 100, 4); 100 max bboxes; 4 = x and y (center coordinates), width, height\n",
    "    #scores: 2D shape (1, 100); detected objects' confidence scores\n",
    "    #classes: 2D shape(1, 100); detected objects' classes\n",
    "    #nums: 1D shape (1); the total number of detected objects\n",
    "    #these variables are important for DeepSORT\n",
    "    \n",
    "    classes = classes[0]\n",
    "    names = []\n",
    "    for i in range(len(classes)):\n",
    "        names.append(class_names[int(classes[i])])\n",
    "    \n",
    "    names = np.array(names) #format for Non-Maximum Suppression (NMS)\n",
    "    numpy_bboxes = np.array(bboxes[0])\n",
    "    converted_bboxes = convert_boxes(frame, numpy_bboxes) #converts boxes into list\n",
    "    features = encoder(frame, converted_bboxes) #generate the feature spectra of the detected object\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature \n",
    "                  in zip(converted_bboxes, scores[0], names, features)]\n",
    "    \n",
    "    #perform non-max suppression to eliminate multiple frames on one target\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores) #indices associate an object with a track\n",
    "    detections = [detections[i] for i in indices] #removes redundancies\n",
    "    \n",
    "    #detections can now be used for DeepSORT since NMS was used to eliminate duplication of the same target\n",
    "    tracker.predict() #uses Kalman filtering\n",
    "    tracker.update(detections) #updates the Kalman tracker parameters and filter\n",
    "    \n",
    "    cmap = plt.get_cmap('tab20b') #generate color maps\n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0,1,20)] #generate 20 steps colors\n",
    "\n",
    "    #show tracked objects\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1: #if Kalman filtering was not able to assign a track\n",
    "            continue\n",
    "        class_name = track.get_class() #get the corresponding classes\n",
    "        color = colors[int(track.track_id) % len(colors)] #assigning the color code\n",
    "        color = [i * 255 for i in color] #color originally ranges from 0 to 1; thus must be converted from 0 to 255\n",
    "        \n",
    "        if class_name in vehicle_classes:\n",
    "            bbox = track.to_tlbr() #for OpenCV output minX, minY, maxX, maxY\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2) #bounding box rectangle\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), \n",
    "                          (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1) #rectangle for text\n",
    "            cv2.putText(frame, class_name + \" - \" + str(track.track_id), (int(bbox[0]), int(bbox[1]-10)), \n",
    "                        0, 0.75, (255,255,255), 2) #display text for class name and Tracking ID\n",
    "\n",
    "\n",
    "            center = (int(((bbox[0]) + (bbox[2]))/2), int(((bbox[1]) + (bbox[3]))/2)) #get center coordinates of bounding box\n",
    "            points[track.track_id].append(center)\n",
    "            \n",
    "            # get overlapping bounding boxes\n",
    "            for track2 in tracker.tracks:\n",
    "                class2_name = track2.get_class()\n",
    "                if class2_name in vehicle_classes:\n",
    "                    bbox2 = track2.to_tlbr()\n",
    "                    if track.track_id != track2.track_id:\n",
    "                            # first bbox\n",
    "                            x1_min = bbox[0]\n",
    "                            y1_min = bbox[1]\n",
    "                            x1_max = bbox[2]\n",
    "                            y1_max = bbox[3]\n",
    "                            # second bbox\n",
    "                            x2_min = bbox2[0]\n",
    "                            y2_min = bbox2[1]\n",
    "                            x2_max = bbox2[2]\n",
    "                            y2_max = bbox2[3]\n",
    "                            # check if the two are not the same detections\n",
    "                            center1 = (int(((x1_min) + (x1_max))/2), int(((y1_min) + (y1_max))/2)) \n",
    "                            center2 = (int(((x2_min) + (x2_max))/2), int(((y2_min) + (y2_max))/2))\n",
    "                            centroid_distance = math.sqrt( ((center2[0]-center1[0])**2) + ((center2[1]-center1[1])**2) )\n",
    "                            if centroid_distance > 100:\n",
    "                                # check for overlaps\n",
    "                                if (x1_min < x2_max and x2_min < x1_max) and (y1_min < y2_max and y2_min < y1_max):\n",
    "                                    overlap_anomaly = True\n",
    "                                    overlap_ids = tuple(sorted((track.track_id, track2.track_id)))\n",
    "                                    overlap_list.append(overlap_ids)\n",
    "                                    overlap_list = list(dict.fromkeys(overlap_list))\n",
    "                                    #print(overlap_list)\n",
    "                                    break\n",
    "                                else: \n",
    "                                    overlap_anomaly = False\n",
    "                    \n",
    "            #compute speed and append to list\n",
    "            start_point = points[track.track_id][0]\n",
    "            end_point = points[track.track_id][-1]\n",
    "            distance = (math.sqrt( ((end_point[0]-start_point[0])**2) + ((end_point[1]-start_point[1])**2) ) / end_point[1]) * 100\n",
    "            time_period = len(points[track.track_id])\n",
    "            speed = distance / time_period\n",
    "            speeds[track.track_id].append(speed)\n",
    "            cv2.putText(frame, \"{:.2f} units/frame\".format(speed), (int(bbox[0]), int(bbox[1]-30)), \n",
    "                        0, 0.75, (0,255,0), 2) #display text for speed of Tracking ID\n",
    "            \n",
    "            #compute acceleration\n",
    "            if len(speeds[track.track_id]) > 4:\n",
    "                current_speed = speeds[track.track_id][-1]\n",
    "                prev_speeds = [speeds[track.track_id][-4], speeds[track.track_id][-3], speeds[track.track_id][-2]]\n",
    "                avg_prev_speed = sum(prev_speeds) / len(prev_speeds)\n",
    "                acceleration = current_speed - avg_prev_speed\n",
    "                accelerations[track.track_id].append(acceleration)\n",
    "                #print(\"Accelerations of {}: {}\".format(track.track_id, accelerations[track.track_id]))\n",
    "                \n",
    "                #detect acceleration anomaly score\n",
    "                if len(speeds[track.track_id]) == 21:\n",
    "                    accelerations_reshaped = np.array(accelerations[track.track_id]).reshape(-1, 1)\n",
    "                    model = ReferenceWindowModel(model_cls=HBOS, window_size=20, sliding_size=10, initial_window_X=accelerations_reshaped) # model initialization\n",
    "                elif len(speeds[track.track_id]) > 22:\n",
    "                    if acceleration != 0:\n",
    "                        acc_processed = preprocessor.fit_transform_partial([acceleration])\n",
    "                        anomaly_score = model.fit_score_partial(acc_processed)\n",
    "                        anomaly_score = postprocessor.fit_transform_partial(anomaly_score)\n",
    "                        calibrated_score = calibrator.fit_transform_partial(anomaly_score)\n",
    "                        #if calibrated_score > 0.95:\n",
    "                        cv2.putText(frame, \"{:.2f}\".format(calibrated_score), (int(bbox[0]), int(bbox[3]+30)), \n",
    "                                    0, 0.75, (0,255,0), 2)\n",
    "                        acc_anomaly_list[track.track_id].append(calibrated_score)\n",
    "            \n",
    "            # get projected direction\n",
    "            if len(points[track.track_id]) > 4:\n",
    "                prev_x_points = [points[track.track_id][-4][0], points[track.track_id][-3][0], points[track.track_id][-2][0]]\n",
    "                prev_y_points = [points[track.track_id][-4][1], points[track.track_id][-3][1], points[track.track_id][-2][1]]\n",
    "                avg_x_points = sum(prev_x_points) / len(prev_x_points)\n",
    "                avg_y_points = sum(prev_y_points) / len(prev_y_points)\n",
    "                prev_point = (int(avg_x_points), int(avg_y_points))\n",
    "                delta_y = (end_point[1] - prev_point[1]) \n",
    "                delta_x = (end_point[0] - prev_point[0])\n",
    "                cv2.arrowedLine(frame, (end_point), (end_point[0]+(delta_x*10),end_point[1]+(delta_y*10)), (0,255,0), 2)\n",
    "            \n",
    "            # begin collision detection on overlapping objects\n",
    "            if  len(overlap_list)>0:\n",
    "                for overlap_objs in overlap_list:\n",
    "                    obj1 = overlap_objs[0]\n",
    "                    obj2 = overlap_objs[1]\n",
    "                    if len(points[obj1]) > 4 and len(points[obj2]) > 4:\n",
    "                        # get line coordinates of obj1\n",
    "                        obj1_curr_point = tuple(points[obj1][-1])\n",
    "                        obj1_prev_x_points = [points[obj1][-4][0], points[obj1][-3][0], points[obj1][-2][0]]\n",
    "                        obj1_prev_y_points = [points[obj1][-4][1], points[obj1][-3][1], points[obj1][-2][1]]\n",
    "                        obj1_avg_x_points = sum(obj1_prev_x_points) / len(obj1_prev_x_points)\n",
    "                        obj1_avg_y_points = sum(obj1_prev_y_points) / len(obj1_prev_y_points)\n",
    "                        obj1_prev_point = (int(obj1_avg_x_points), int(obj1_avg_y_points))\n",
    "                        obj1_delta_y = (obj1_curr_point[1] - obj1_prev_point[1]) \n",
    "                        obj1_delta_x = (obj1_curr_point[0] - obj1_prev_point[0])\n",
    "                        obj1_next_point = (obj1_curr_point[0]+(obj1_delta_x*10), obj1_curr_point[1]+(obj1_delta_y*10))\n",
    "                        line1 = [obj1_curr_point, obj1_next_point]\n",
    "                        # get line coordinates of obj2\n",
    "                        obj2_curr_point = tuple(points[obj2][-1])\n",
    "                        obj2_prev_x_points = [points[obj2][-4][0], points[obj2][-3][0], points[obj2][-2][0]]\n",
    "                        obj2_prev_y_points = [points[obj2][-4][1], points[obj2][-3][1], points[obj2][-2][1]]\n",
    "                        obj2_avg_x_points = sum(obj2_prev_x_points) / len(obj2_prev_x_points)\n",
    "                        obj2_avg_y_points = sum(obj2_prev_y_points) / len(obj2_prev_y_points)\n",
    "                        obj2_prev_point = (int(obj2_avg_x_points), int(obj2_avg_y_points))\n",
    "                        obj2_delta_y = (obj2_curr_point[1] - obj2_prev_point[1]) \n",
    "                        obj2_delta_x = (obj2_curr_point[0] - obj2_prev_point[0])\n",
    "                        obj2_next_point = (obj2_curr_point[0]+(obj2_delta_x*10), obj2_curr_point[1]+(obj2_delta_y*10))\n",
    "                        line2 = [obj2_curr_point, obj2_next_point]\n",
    "                        # get intersection of the two lines\n",
    "                        intersection = line_intersection(line1,line2)\n",
    "                        if intersection[0] != None and intersection[1] != None:\n",
    "                            cv2.circle(frame, (intersection[0],intersection[1]), 10, (0,0,255), -1) #visualize intersection\n",
    "                            \n",
    "                            if len(speeds[obj1]) > 22 and len(speeds[obj2]) > 22: #minimum number of speed values for anomaly detection\n",
    "                                if acc_anomaly_list[obj1][-1] > 0.9 and acc_anomaly_list[obj2][-1] > 0.9: #threshold of anomaly score\n",
    "                                    cv2.putText(frame, \"Collision Detected!\", (30, frame_height-30), 0, 1, (0,0,255), 2)\n",
    "            \n",
    "            #for historical trajectory\n",
    "            for j in range(1, len(points[track.track_id])):\n",
    "                if points[track.track_id][j-1] is None or points[track.track_id][j] is None: #check if current and previous tracker has a center point\n",
    "                    continue\n",
    "                thickness = int(np.sqrt(64/float(j+1))*2) #closer points are visually thinner\n",
    "                cv2.line(frame, (points[track.track_id][j-1]), (points[track.track_id][j]), color, thickness)\n",
    "\n",
    "    processing_times.append(t1 - t2)\n",
    "    #use processing times from last 200 frames\n",
    "    if len(processing_times) > 200:\n",
    "        processing_times.popleft()\n",
    "\n",
    "    _, f_width = frame.shape[:2]\n",
    "    # get mean processing time [ms]\n",
    "    processing_time = np.mean(processing_times) * 1000\n",
    "    #get fps\n",
    "    fps = 1000 / processing_time\n",
    "    #display fps\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        f\"Inference Time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "        (20, 40),\n",
    "        cv2.FONT_HERSHEY_COMPLEX,\n",
    "        f_width / 1000,\n",
    "        (0,0,255),\n",
    "        1,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    #display date and time\n",
    "    current_time = time.asctime( time.localtime(time.time()) )\n",
    "    cv2.putText(frame, current_time, (0,80), 0, 1, (255,255,255), 2)\n",
    "\n",
    "    #display video stream\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #press Q to quit\n",
    "        break\n",
    "\n",
    "#clear stream capture\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ec973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
