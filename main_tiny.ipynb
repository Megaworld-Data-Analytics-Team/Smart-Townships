{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d68506",
   "metadata": {},
   "source": [
    "## Person and Vehicle Counter using OpenCV and YOLOv3 Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd9397",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596ea3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For command line use of YOLOv3 (required)\n",
    "from absl import flags\n",
    "import sys\n",
    "FLAGS = flags.FLAGS\n",
    "sys.argv = sys.argv[:1]\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #use CPU=-1, GPU=0\n",
    "\n",
    "import time #for calculating FPS\n",
    "import numpy as np\n",
    "import cv2 #OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "#under yolov3_tf2 folder\n",
    "from yolov3_tf2.models import YoloV3Tiny\n",
    "from yolov3_tf2.dataset import transform_images #for data augmentation\n",
    "from yolov3_tf2.utils import convert_boxes #converts bboxes to deepsort format\n",
    "\n",
    "#under deep_sort folder\n",
    "from deep_sort import preprocessing #for max suppressions\n",
    "from deep_sort import nn_matching #for setting up the association metrics\n",
    "from deep_sort.detection import Detection #for object detection\n",
    "from deep_sort.tracker import Tracker #for object tracking information\n",
    "from tools import generate_detections as gdet #feature generation encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474100d0",
   "metadata": {},
   "source": [
    "### Load YOLOv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4396c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x149734dcaf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define classes\n",
    "class_names = [c.strip() for c in open('./data/labels/coco.names').readlines()]\n",
    "\n",
    "#define allowed classes:\n",
    "allowed_classes = ['person', 'bicycle', 'car', 'motorbike', 'bus', 'truck']\n",
    "\n",
    "#load model\n",
    "yolo = YoloV3Tiny(classes=len(class_names))\n",
    "yolo.load_weights('./weights/yolov3-tiny.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9638",
   "metadata": {},
   "source": [
    "### Initialize DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df36ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.5 #used to determine if objects between frames are the same\n",
    "nn_budget = None #used to form a gallery for storing of features\n",
    "nms_max_overlap = 0.8 #used to avoid too many detections on the same object\n",
    "\n",
    "model_filename = './model_data/mars-small128.pb' #pretrained CNN for pedestrian tracking\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1) #feature generations\n",
    "\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget) #for measuring associations\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a892af",
   "metadata": {},
   "source": [
    "### Track Using Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7deab064",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/test.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID') #constructs the fourcc code of the codec for the VideoWriter constructor\n",
    "vid_fps = int(vid.get(cv2.CAP_PROP_FPS)) #fps of original video CAP_PROP_FPS returns float\n",
    "vid_width, vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "output_vid = cv2.VideoWriter('./data/video/results-tiny.avi', codec, vid_fps, (vid_width,vid_height)) #saves results to an output video\n",
    "\n",
    "#list for historical trajectory\n",
    "from collections import deque\n",
    "points = [deque(maxlen=30) for _ in range(1000)]\n",
    "\n",
    "#for counting\n",
    "person_counter = []\n",
    "vehicle_counter = []\n",
    "\n",
    "#while loop for capturing all the frames in the video\n",
    "while True:\n",
    "    _, frame = vid.read()\n",
    "    \n",
    "    if frame is None: #if reaches the end of the video and there is no more image\n",
    "        print('Completed!')\n",
    "        break\n",
    "    \n",
    "    #preprocessing for YOLOv3 Input\n",
    "    frame_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #video captured by OpenCV is in BGR format; tensorflow is RGB\n",
    "    frame_input = tf.expand_dims(frame_input, 0) #expands dims from C,H,W to N,C,H,W\n",
    "    frame_input = transform_images(frame_input, 416) #tensorflow shape is 416\n",
    "\n",
    "    t1 = time.time() #start the timer\n",
    "    \n",
    "    bboxes, scores, classes, nums = yolo.predict(frame_input)\n",
    "    \n",
    "    #maximum of 100 bboxes per image\n",
    "    #boxes: 3D shape (1, 100, 4); 100 max bboxes; 4 = x and y (center coordinates), width, height\n",
    "    #scores: 2D shape (1, 100); detected objects' confidence scores\n",
    "    #classes: 2D shape(1, 100); detected objects' classes\n",
    "    #nums: 1D shape (1); the total number of detected objects\n",
    "    #these variables are important for DeepSORT\n",
    "    \n",
    "    classes = classes[0]\n",
    "    names = []\n",
    "    for i in range(len(classes)):\n",
    "        names.append(class_names[int(classes[i])])\n",
    "    \n",
    "    names = np.array(names) #format for Non-Maximum Suppression (NMS)\n",
    "    converted_bboxes = convert_boxes(frame, bboxes[0]) #converts boxes into list\n",
    "    features = encoder(frame, converted_bboxes) #generate the feature spectra of the detected object\n",
    "    \n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature \n",
    "                  in zip(converted_bboxes, scores[0], names, features)]\n",
    "    \n",
    "    #perform non-max suppression to eliminate multiple frames on one target\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores) #indices associate an object with a track\n",
    "    detections = [detections[i] for i in indices] #removes redundancies\n",
    "    \n",
    "    #detections can now be used for DeepSORT since NMS was used to eliminate duplication of the same target\n",
    "    tracker.predict() #uses Kalman filtering\n",
    "    tracker.update(detections) #updates the Kalman tracker parameters and filter\n",
    "    \n",
    "    cmap = plt.get_cmap('tab20b') #generate color maps\n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0,1,20)] #generate 20 steps colors \n",
    "    \n",
    "    person_current_count = int(0) #detect current vehicle in specific zone\n",
    "    vehicle_current_count = int(0) #detect current vehicles in specific zone\n",
    "    \n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1: #if Kalman filtering was not able to assign a track\n",
    "            continue\n",
    "            \n",
    "        bbox = track.to_tlbr() #for OpenCV output minX, minY, maxX, maxY\n",
    "        class_name = track.get_class() #get the corresponding classes\n",
    "        color = colors[int(track.track_id) % len(colors)] #assigning the color code\n",
    "        color = [i * 255 for i in color] #color originally ranges from 0 to 1; thus must be converted from 0 to 255\n",
    "        \n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2) #bounding box rectangle\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), \n",
    "                      (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1) #rectangle for text\n",
    "        cv2.putText(frame, class_name + \" - \" + str(track.track_id), (int(bbox[0]), int(bbox[1]-10)), \n",
    "                    0, 0.75, (255,255,255), 2) #display text for class name and Tracking ID\n",
    "        \n",
    "        center = (int(((bbox[0]) + (bbox[2]))/2), int(((bbox[1]) + (bbox[3]))/2)) #get center coordinates of bounding box\n",
    "        points[track.track_id].append(center)\n",
    "        \n",
    "        #for historical trajectory\n",
    "        for j in range(1, len(points[track.track_id])):\n",
    "            if points[track.track_id][j-1] is None or points[track.track_id][j] is None: #check if current and previous tracker has a center point\n",
    "                continue\n",
    "            thickness = int(np.sqrt(64/float(j+1))*2) #closer points are visually thinner\n",
    "            cv2.line(frame, (points[track.track_id][j-1]), (points[track.track_id][j]), color, thickness)\n",
    "        \n",
    "        #for counter in zone\n",
    "        height, width, _ = frame.shape\n",
    "        #zone\n",
    "        cv2.line(frame, (0, int(3*height/6 + height/20)), (width, int(3*height/6 + height/20)), (0,255,0), thickness=2)\n",
    "        cv2.line(frame, (0, int(3*height/6 - height/20)), (width, int(3*height/6 - height/20)), (0,255,0), thickness=2)\n",
    "        \n",
    "        center_x = int(((bbox[0])+(bbox[2]))/2)\n",
    "        center_y = int(((bbox[1])+(bbox[3]))/2)\n",
    "        \n",
    "        if center_y <= int(3*height/6 + height/20) and center_y >= int(3*height/6 - height/20):\n",
    "            if class_name == allowed_classes[0]: #if detected class is a person\n",
    "                person_counter.append(int(track.track_id))\n",
    "                person_current_count += 1\n",
    "            elif class_name in allowed_classes[1:]: #if detected class is a vehicle\n",
    "                vehicle_counter.append(int(track.track_id))\n",
    "                vehicle_current_count += 1\n",
    "    \n",
    "    #display persons count\n",
    "    person_total_count = len(set(person_counter))\n",
    "    cv2.putText(frame, \"Current Persons in Detection Zone: \" + str(person_current_count), (0,80), 0, 1, (0,255,0),2)\n",
    "    cv2.putText(frame, \"Total Persons Count: \" + str(person_total_count), (0,180), 0, 1, (0,255,0),2)\n",
    "    \n",
    "    #display vehicle count\n",
    "    vehicle_total_count = len(set(vehicle_counter))\n",
    "    cv2.putText(frame, \"Current Vehicles in Detection Zone: \" + str(vehicle_current_count), (0,130), 0, 1, (0,255,0),2)\n",
    "    cv2.putText(frame, \"Total Vehicle Count: \" + str(vehicle_total_count), (0,230), 0, 1, (0,255,0),2)\n",
    "    \n",
    "    #display FPS\n",
    "    fps = 1./(time.time() - t1)\n",
    "    cv2.putText(frame, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "    \n",
    "    cv2.imshow('output', frame)\n",
    "    \n",
    "    output_vid.write(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27: #press ESC to quit video\n",
    "        break\n",
    "        \n",
    "vid.release()\n",
    "output_vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe54f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
